# LM-Reasoning-Papers

> A list of papers related to arithmetic, commonsense and symbolic reasoning using Language Models.

<img src="https://img.shields.io/badge/Contributions-Welcome-278ea5" alt="Contrib"/> <img src="https://img.shields.io/badge/Last%20Update-2022--11--05-success" alt="update"/> <img src="https://img.shields.io/badge/Number%20of%20Papers-25-2D333B" alt="PaperNum"/>

# <img src="https://img.shields.io/badge/Paper%20Type-Methods-informational" alt="type"/>

- Large Language Models Can Self-Improve [[paper]](https://arxiv.org/abs/2210.11610)
- Scaling instruction-finetuned language models [[paper]](https://arxiv.org/abs/2210.11416)
- Challenging BIG-Bench tasks and whether chain-of-thought can solve them [[paper]](https://arxiv.org/abs/2210.09261)
- Mind's Eye: Grounded language model reasoning through simulation [[paper]](https://arxiv.org/abs/2210.05359)
- Automatic Chain of Thought Prompting in Large Language Models [[paper]](https://arxiv.org/abs/2210.03493)
- Language models are multilingual chain-of-thought reasoners [[paper]](https://arxiv.org/abs/2210.03057)
- Finetuned Language Models Are Zero-Shot Learners [[paper]](https://arxiv.org/abs/2109.01652)
- On the Advance of Making Language Models Better Reasoners [[paper]](https://arxiv.org/pdf/2206.02336)
- Least-to-most prompting enables complex reasoning in large language models [[paper]](https://arxiv.org/abs/2205.10625)
- Large Language Models are Zero-Shot Reasoners [[paper]](https://arxiv.org/abs/2205.11916)
- Self-consistency improves chain of thought reasoning in language models [[paper]](https://arxiv.org/abs/2203.11171)
- `NEURIPS22` Chain of Thought Prompting Elicits Reasoning in Large Language Models [[paper]](https://arxiv.org/abs/2201.11903)

# <img src="https://img.shields.io/badge/Paper%20Type-Dataset-red" alt="conf"/>

- Training Verifiers to Solve Math Word Problems [[paper]](https://arxiv.org/abs/2110.14168) [[code]](https://github.com/openai/grade-school-math)
- `NAACL21` Are NLP Models really able to Solve Simple Math Word Problems? [[paper]](https://arxiv.org/abs/2103.07191) [[code]](https://github.com/arkilpatel/SVAMP)
- `ACL20` A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers [[paper]](https://arxiv.org/abs/2106.15772) [[code]](https://github.com/chaochun/nlu-asdiv-dataset)
- `TACL21` Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies [[paper]](https://arxiv.org/abs/2101.02235) [[code]](https://github.com/eladsegal/strategyqa)
- `NAACL19` CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge [[paper]](https://arxiv.org/abs/1811.00937) [[code]](https://github.com/jonathanherzig/commonsenseqa)
- Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge [[paper]](https://arxiv.org/abs/1803.05457)
- Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems [[paper]](https://arxiv.org/abs/1705.04146) [[code]](https://github.com/deepmind/AQuA)
- `NAACL16` MAWPS: A Math Word Problem Repository [[paper]](https://aclanthology.org/N16-1136/) [[code]](https://github.com/sroy9/mawps)
- `EMNLP15` Solving General Arithmetic Word Problems [[paper]](https://arxiv.org/abs/1608.01413)
- `EMNLP14` Learning to Solve Arithmetic Word Problems with Verb Categorization [[paper]](https://aclanthology.org/D14-1058/)

# <img src="https://img.shields.io/badge/Paper%20Type-Framework-brightgreen" alt="arXiv"/>

- `EMNLP22` Retrieval Augmentation for Commonsense Reasoning: A Unified Approach [[paper]](https://arxiv.org/pdf/2210.12887) [[code]](https://github.com/wyu97/RACo)
- Large Language Models Still Can't Plan [[paper]](https://arxiv.org/abs/2206.10498)

# <img src="https://img.shields.io/badge/Paper%20Type-Survey-FFD700" alt="type"/>

- `TMLR22` Emergent Abilities of Large Language Models [[paper]](https://arxiv.org/abs/2206.07682)

## Copyright

This repository was inspired by [this paper reading list](https://github.com/Sahandfer/EMPaper).

The contents of this repository are bound by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) license.
